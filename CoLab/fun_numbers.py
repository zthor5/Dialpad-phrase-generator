# -*- coding: utf-8 -*-
"""Fun Numbers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HBF2dP2_PhrRXRBqrVf7qL6mIqP30vBQ
"""

!pip install -q google-genai

# ######################################################################################
# VCF PHONE NUMBER PHRASE GENERATOR
# ######################################################################################
#
# This program processes contact files (.vcf) to find memorable phone number phrases
# using Gemini 2.5 Pro with "thinking mode". Given a VCF contact file, it:
#   1. Parses all contacts and extracts phone numbers
#   2. For each phone number, generates creative memorable phrases based on telephone keypad letters
#   3. Processes multiple phone numbers concurrently (10 at a time) for efficiency
#   4. Analyzes all results to determine the "best" or most memorable phone number phrase
#
# The code uses a modular design pattern with an abstracted LLM interface that could be
# swapped for other AI providers. It leverages Google Colab's asynchronous capabilities
# and direct support for top-level await statements.
#
# ######################################################################################

# Import necessary libraries
import re                           # For regular expressions (parsing VCF)
import os                           # For file operations
import time                         # For timing and delays
import base64                       # For encoding/decoding if needed
import asyncio                      # For async/await functionality
from typing import List, Dict, Any, Tuple  # For type annotations
from google import genai            # Google's Generative AI library
from google.genai import types      # Type definitions for Google GenAI
from google.colab import userdata   # For accessing secure API keys in Colab

# ######################################################################################
# BASE LLM ARCHITECTURE - Provides abstracted interface for language model interactions
# ######################################################################################

class BaseLLM:
    """
    Abstract base class for language model interactions.

    This class defines the interface that all LLM implementations must follow,
    making it easy to swap between different AI providers or models.
    """

    def __init__(self):
        """Initialize the LLM base class."""
        pass

    def generate(self, prompt: str) -> str:
        """
        Generate a response for the given prompt.

        Args:
            prompt: The text prompt to send to the LLM

        Returns:
            The generated response as text

        Raises:
            NotImplementedError: This is an abstract method that must be implemented by subclasses
        """
        raise NotImplementedError("Subclasses must implement this method")


class GeminiLLM(BaseLLM):
    """
    Implementation of the LLM interface using Google's Gemini API with thinking mode.

    This class handles:
    - API key management (from Colab userdata)
    - Configuring Gemini with thinking mode
    - Both synchronous and asynchronous generation
    """

    def __init__(self, model_name: str = "gemini-2.5-pro-preview-03-25", thinking_budget: int = 2048):
        """
        Initialize the Gemini LLM with specified model and thinking budget.

        Args:
            model_name: The specific Gemini model to use
            thinking_budget: The token budget for Gemini's thinking feature (higher = more thorough reasoning)
        """
        super().__init__()
        self.model_name = model_name
        self.thinking_budget = thinking_budget
        self._setup_client()

    def _setup_client(self):
        """
        Set up the Gemini client with API key from userdata or environment.

        This method tries to get the API key from Colab's userdata first (more secure),
        then falls back to environment variables if needed.

        Raises:
            ValueError: If no API key can be found
        """
        # Get API key directly from Google Colab userdata
        try:
            api_key = userdata.get('api-key')
        except Exception as e:
            print(f"Error accessing userdata: {str(e)}")
            print("Trying environment variable.")
            api_key = os.environ.get("GEMINI_API_KEY")

        if not api_key:
            raise ValueError("No API key found. Please provide a Gemini API key through Colab userdata.")

        # Store the API key for later use
        self.api_key = api_key
        print(f"Successfully retrieved API key from Colab userdata. Using model: {self.model_name}")

    def generate(self, prompt: str) -> str:
        """
        Generate a response using Gemini's API with thinking enabled (synchronous version).

        This method:
        1. Creates a new client instance with the API key
        2. Configures the thinking budget
        3. Calls the Gemini API
        4. Extracts and returns the text response

        Args:
            prompt: The text prompt to send to Gemini

        Returns:
            The generated response as text

        Raises:
            Prints error message and returns error string if API call fails
        """
        try:
            print(f"Sending prompt to Gemini with thinking_budget={self.thinking_budget}")

            # Create a client instance
            client = genai.Client(api_key=self.api_key)

            # Configure response settings with thinking mode enabled
            generate_content_config = types.GenerateContentConfig(
                response_mime_type="text/plain",
                thinking_config=types.ThinkingConfig(thinking_budget=self.thinking_budget)
            )

            # Call the model
            response = client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=generate_content_config
            )

            # Return the response text
            if hasattr(response, 'text'):
                return response.text
            elif hasattr(response, 'parts') and len(response.parts) > 0:
                return response.parts[0].text
            else:
                return str(response)

        except Exception as e:
            error_msg = f"API Error: {str(e)}"
            print(error_msg)
            return error_msg

    async def generate_async(self, prompt: str) -> str:
        """
        Generate a response using Gemini's API with thinking enabled (async version).

        This method works like the synchronous version but can be awaited in async contexts,
        allowing for concurrent processing of multiple requests.

        Args:
            prompt: The text prompt to send to Gemini

        Returns:
            The generated response as text

        Raises:
            Prints error message and returns error string if API call fails
        """
        try:
            # Create a client instance
            client = genai.Client(api_key=self.api_key)

            # Configure response settings with thinking mode enabled
            generate_content_config = types.GenerateContentConfig(
                response_mime_type="text/plain",
                thinking_config=types.ThinkingConfig(thinking_budget=self.thinking_budget)
            )

            # Call the model
            response = client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=generate_content_config
            )

            # Return the response text
            if hasattr(response, 'text'):
                return response.text
            elif hasattr(response, 'parts') and len(response.parts) > 0:
                return response.parts[0].text
            else:
                return str(response)

        except Exception as e:
            error_msg = f"API Error: {str(e)}"
            print(error_msg)
            return error_msg


# ######################################################################################
# VCF PARSER - Extracts contact information from VCF (vCard) files
# ######################################################################################

def parse_vcf_file(vcf_path: str) -> List[Dict[str, str]]:
    """
    Parse a .vcf file and extract contacts with phone numbers.

    This function:
    1. Reads the VCF file
    2. Splits it into individual vCard blocks
    3. Extracts name and phone numbers from each block
    4. Formats phone numbers consistently as XXX-XXX-XXXX

    Args:
        vcf_path: Path to the .vcf file

    Returns:
        List of dictionaries, each with 'name' and 'phone' keys

    Raises:
        FileNotFoundError: If the VCF file doesn't exist (handled by caller)
    """
    contacts = []
    print(f"Opening VCF file at {vcf_path}")

    # Read the entire VCF file
    with open(vcf_path, 'r', encoding='utf-8', errors='ignore') as file:
        vcf_content = file.read()

    print(f"Read {len(vcf_content)} bytes from file")

    # Split the file into individual vCards
    vcard_blocks = re.split(r'(?:\r\n|\n)BEGIN:VCARD', vcf_content)
    print(f"Found {len(vcard_blocks)} vCard blocks in file")

    # Process each vCard block
    for block in vcard_blocks:
        if not block.strip():
            continue

        # Make sure the block starts with BEGIN:VCARD
        if not block.upper().startswith('BEGIN:VCARD'):
            block = 'BEGIN:VCARD' + block

        # Extract name (try FN field first, then N field)
        name_match = re.search(r'(?:^|\n)FN:(.*?)(?:\r\n|\n)', block)
        if not name_match:
            name_match = re.search(r'(?:^|\n)N:(.*?)(?:\r\n|\n)', block)

        name = name_match.group(1).strip() if name_match else "Unknown Contact"

        # Extract phone numbers (TEL fields)
        phone_matches = re.finditer(r'(?:^|\n)TEL(?:[^:]*):([^\r\n]+)', block)

        phones = []
        for phone_match in phone_matches:
            phone = phone_match.group(1).strip()
            # Clean up and format the phone number (remove non-digits)
            clean_number = re.sub(r'\D', '', phone)
            if len(clean_number) >= 10:
                # If number has country code, take just the last 10 digits
                clean_number = clean_number[-10:]
                # Format as XXX-XXX-XXXX
                formatted_number = f"{clean_number[:3]}-{clean_number[3:6]}-{clean_number[6:]}"
                phones.append(formatted_number)

        # Add each phone number as a separate contact entry
        for phone in phones:
            contacts.append({
                "name": name,
                "phone": phone
            })

    return contacts


# ######################################################################################
# PHONE NUMBER PROCESSING - Asynchronously processes multiple phone numbers
# ######################################################################################

async def process_contact_phones_async(contacts: List[Dict[str, str]], llm: BaseLLM, batch_size: int = 10) -> List[Tuple[Dict[str, str], str]]:
    """
    Process each contact's phone number through the LLM asynchronously in batches.

    This function:
    1. Divides contacts into batches of specified size
    2. For each batch, creates async tasks for LLM generation
    3. Awaits all tasks in a batch before moving to the next
    4. Collects and returns all results

    Args:
        contacts: List of contact dictionaries with name and phone
        llm: LLM instance to use for generation
        batch_size: Number of concurrent API calls to make

    Returns:
        List of tuples, each containing (contact_info, api_response)
    """
    results = []
    total_contacts = len(contacts)

    # Process contacts in batches
    for i in range(0, total_contacts, batch_size):
        batch = contacts[i:i+batch_size]
        print(f"Processing batch {i//batch_size + 1} of {(total_contacts + batch_size - 1) // batch_size} ({len(batch)} numbers)")

        # Prepare tasks for current batch
        async_tasks = []
        for contact in batch:
            name = contact.get("name", "Unknown Contact")
            phone = contact.get("phone", "")

            if not phone:
                continue

            print(f"Queuing phone number for {name}: {phone}")

            # Create prompt for this phone number
            prompt = f"""
Generate memorable phrases for the phone number {phone}. Base the phrases on the letters corresponding to each digit on a standard telephone keypad.
 Rules:
- First, reason out each viable option. Run through multiple scenarios. Then refine the best options.
    -   Only use 'O' or 0 in each generation, do not mix the two.
    - You can have only a portion of the phone number be a word or phrase (Ex. 800-PLUMBER)
    - Only respond to the user with the best viable options that only use 10 letters/numbers.
    - Respond with "No good options" if there are no distinct phrases.
"""
            # Add task to list
            task = asyncio.create_task(llm.generate_async(prompt))
            async_tasks.append((contact, task))

        # Wait for all tasks in this batch to complete
        for contact, task in async_tasks:
            response = await task
            results.append((contact, response))
            print(f"Response received for {contact.get('name', 'Unknown')}")

        # Small delay between batches to avoid any rate limiting
        if i + batch_size < total_contacts:
            print(f"Batch complete. Waiting before processing next batch...")
            await asyncio.sleep(2)

    return results


def get_best_result_prompt(results: List[Tuple[Dict[str, str], str]]) -> str:
    """
    Create prompt for the final evaluation that compares all phone number phrases.

    This function:
    1. Compiles all contact names, phones, and responses into a context
    2. Creates a prompt asking for the most memorable/fun option

    Args:
        results: List of (contact_info, api_response) tuples

    Returns:
        A complete prompt string for the final evaluation
    """
    # Create context from all results
    context = "\n\n".join([f"Contact: {contact.get('name', 'Unknown')}\nPhone: {contact.get('phone', '')}\nResponse: {response}"
                        for contact, response in results])

    # Create final prompt
    prompt = f"""
Based on all of the phone numbers and matching phrases based on the letters corresponding to each digit on a standard telephone keypad, decide which is the most memorable or fun.

In your response, be sure to include:
1. The winning phone number
2. The name of the contact associated with this phone number
3. The memorable phrase for this number
4. A brief explanation of why this is the best one

Context: {context}
"""

    return prompt


# ######################################################################################
# MAIN PROCESSING FUNCTION - Orchestrates the entire workflow
# ######################################################################################

async def fun_numbers_async(vcf_path: str):
    """
    Process a VCF file to find fun phone number phrases asynchronously.

    This function orchestrates the entire workflow:
    1. Validates the input file exists
    2. Initializes LLM instances with appropriate thinking budgets
    3. Parses contact information from the VCF
    4. Processes phone numbers concurrently in batches
    5. Evaluates all results to find the best phrase
    6. Displays both individual and overall results

    Args:
        vcf_path: Path to the .vcf contact file

    Returns:
        Tuple of (results, best_result) where:
            - results is a list of (contact_info, api_response) tuples
            - best_result is the final evaluation string
    """
    print("\n" + "="*50)
    print("PHONE NUMBER PHRASE GENERATOR")
    print("="*50)

    # Step 1: Validate file exists
    if not os.path.exists(vcf_path):
        print(f"Error: File {vcf_path} not found.")
        return None, "File not found"

    # Step 2: Initialize our LLM for phone number processing (smaller thinking budget)
    print("\nStep 1: Initializing Gemini LLM with thinking mode (2048 tokens)...")
    phone_llm = GeminiLLM(model_name="gemini-2.5-pro-preview-03-25", thinking_budget=2048)#2048

    # Step 3: Parse VCF file
    print("\nStep 2: Parsing VCF file...")
    contacts = parse_vcf_file(vcf_path)
    print(f"Found {len(contacts)} contacts with phone numbers")

    # Step 4: Process phone numbers asynchronously (10 at a time)
    print("\nStep 3: Processing phone numbers through Gemini API (10 concurrent calls)...")
    results = await process_contact_phones_async(contacts, phone_llm, batch_size=100) # 40

    # Step 5: Create a second LLM instance with higher thinking budget for the evaluation
    print("\nStep 4: Initializing Gemini LLM for final evaluation (20000 tokens)...")
    evaluation_llm = GeminiLLM(model_name="gemini-2.5-pro-preview-03-25", thinking_budget=20000)

    # Step 6: Get best result (this runs after all phone numbers are processed)
    print("Finding the best phone number phrase...")
    best_result = evaluation_llm.generate(get_best_result_prompt(results))

    # Step 7: Print results
    print("\n" + "="*50)
    print("INDIVIDUAL RESULTS:")
    print("="*50)
    for contact, response in results:
        print(f"\nContact: {contact.get('name', 'Unknown')}")
        print(f"Phone: {contact.get('phone', '')}")
        print("-"*30)
        print(response)
        print("-"*30)

    print("\n" + "="*50)
    print("BEST RESULT:")
    print("="*50)
    print(best_result)

    print("\nProcess completed successfully!")

    return results, best_result


# ######################################################################################
# EXECUTION - Set your API key and run the code
# ######################################################################################

# To set your API key securely (run this once):
# userdata.set('api-key', 'YOUR_GEMINI_API_KEY')

# Set the path to your VCF file
vcf_path = "./contacts.vcf"

# Run the async function directly (direct await is supported in Colab cells)
results, best_result = await fun_numbers_async(vcf_path)